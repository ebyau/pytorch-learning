{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE REGRESSION PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (80, 1)\n",
      "Validation data:  (20, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "true_w = 2\n",
    "true_b = 1\n",
    "N =100\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = 0.1 * np.random.randn(N, 1)\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# shuffle indicies\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Training and validation idx\n",
    "train_idx = idx[:int(N*0.80)]\n",
    "val_idx = idx[int(N*0.80):]\n",
    "\n",
    "# Split the data\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n",
    "# print out shape of data\n",
    "print('Training data: ', x_train.shape)\n",
    "print('Validation data: ', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model parameters b w:  [0.37454012] [0.95071431]\n",
      "Final model parameters b w:  [1.08425392] [1.85016647]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# initialize the model parameters\n",
    "b = np.random.rand(1)\n",
    "w = np.random.rand(1)\n",
    "\n",
    "print(\"Initial model parameters b w: \", b, w)\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # compute model's prediction\n",
    "    y_hat = w * x_train + b\n",
    "\n",
    "    # Compute the error\n",
    "    error = (y_hat - y_train)\n",
    "    #compute the mean square error\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # compute the gradient\n",
    "    dw = 2* (x_train * error).mean()\n",
    "    db = 2 * error.mean()\n",
    "\n",
    "    # update the parameters w and b\n",
    "    b = b - lr*db\n",
    "    w = w - lr*dw\n",
    "\n",
    "print(\"Final model parameters b w: \", b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn\n",
    "Compare with coefficients from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train,y_train)\n",
    "print(lr_model.intercept_, lr_model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Device\n",
    "Check whether you have GPU or CPU. GPU is normally used to speed up operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to convert the numpy code used earlier into Pytorch. Starting with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Parameters\n",
    "Tensors used as trainable weights or paramaters require computation of their gradients, to update their values. Therefore `requires_grad` argument is added to tell pytorch to compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True)\n",
      "tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize param b and w\n",
    "torch.manual_seed(42)\n",
    "\n",
    "b = torch.randn(1,requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True,dtype=torch.float,device=device)\n",
    "\n",
    "print(b)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "Pytorch's Autograd function will compute the gradient of the w and b paramaters. To avoid gradient accumulation we clear the gradients after every epoch using `grad.zero_` method.  \n",
    "`nn.MSELoss()` is not loss function itself, so can pass labels and targets, however it returns another function `loss_fn` wherewe can pass labels and targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1552], requires_grad=True) tensor([1.7113], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# initialize the SGD Optimizer\n",
    "optimizer = optim.SGD([b,w],lr = lr)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # model prediction\n",
    "    y_hat = w * x_train_tensor + b\n",
    "\n",
    "    # # compute the error between prediction and actual label\n",
    "    # error = (y_hat - y_train_tensor) \n",
    "    # # MSE\n",
    "    # loss = (error ** 2).mean()\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_hat, y_train_tensor)\n",
    "\n",
    "\n",
    "    # Compute gradients for w and b\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters w and b\n",
    "    optimizer.step()\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    # # clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    #     b.grad.zero_()\n",
    "    #     w.grad.zero_()\n",
    "    \n",
    "\n",
    "print(b,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Manual Model creating parameter by parameter\n",
    "To make `b` and `w` parameters associated with the model,we wrapped them around the `Parameter` class. In so doing, now `b` and `w` are now associated as parameters of the model created using `ManualLinearRegression` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # wrapping w and b with nn.Parameter makes them real\n",
    "        # parameters of the Model\n",
    "        self.b = nn.Parameter(torch.randn(1,requires_grad=True,\n",
    "                                          dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True,\n",
    "                                          dtype=torch.float))\n",
    "        \n",
    "    def forward(self,x):\n",
    "            # compute model predictions\n",
    "            return self.b + self.w * x\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the newly created model for making predictions\n",
    "The models and the data all have to reside in the same device, therefore after creating the model we send it to the device.  \n",
    "Since we defined w and b as model parameters using `Parameters` when doing the updates using optimizer we can retrieve them using `model.parameters()`.  \n",
    "`model.train()` does not actually train the model but sets it to trainable state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.1552])), ('w', tensor([1.7113]))])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "# Initialize param w and b\n",
    "torch.manual_seed(42)\n",
    "# Create model and send it to device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# Define the optimizer to update paramters\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "# define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    # model predictions\n",
    "    y_hat = model(x_train_tensor)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_fn(y_hat, y_train_tensor)\n",
    "\n",
    "    # compute gradients for w and b\n",
    "    loss.backward()\n",
    "\n",
    "    # Update paramters\n",
    "    optimizer.step()\n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# inspect models parameters using state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pytorch's Linear Model\n",
    "The regression problem we dealing with consists on one input and one output. Instead of Manually defining parameters `b` and `w`, we can use `linear` model class from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial:  \n",
      "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n",
      "Finally\n",
      "OrderedDict([('linear.weight', tensor([[1.7777]])), ('linear.bias', tensor([1.1213]))])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1 \n",
    "torch.manual_seed(42)\n",
    "loss_fn = nn.MSELoss()\n",
    "epochs = 100\n",
    "model = MyLinearRegression().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Initial Model Parameters  \")\n",
    "print(model.state_dict())\n",
    "for epoch in range(epochs):\n",
    "    # declare model trainanle\n",
    "    model.train()\n",
    "    # model predictions\n",
    "    y_hat = model(x_train_tensor)\n",
    "    # compute the loss\n",
    "    loss = loss_fn(y_hat, y_train_tensor)\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "    # update model paramters\n",
    "    optimizer.step()\n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "print('Final Model Paramters')\n",
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kigali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
